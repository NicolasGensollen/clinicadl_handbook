{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6647de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell if running in Google Colab\n",
    "!pip install clinicadl==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a38535",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Perfom classification using pretrained models\n",
    "\n",
    "<SCRIPT SRC='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></SCRIPT>\n",
    "<SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})</SCRIPT> \n",
    "\n",
    "This notebook shows how to perform classification on preprocessed data using pretrained models described in ([Wen et al, 2020](https://www.sciencedirect.com/science/article/abs/pii/S1361841520300591)).\n",
    "\n",
    "## Structure of the pretrained models\n",
    "\n",
    "All the pretrained model folders are organized as follows:\n",
    "<pre>\n",
    "<b>results</b>\n",
    "├── commandline.json\n",
    "├── <b>fold-0</b>\n",
    "├── ...\n",
    "└── <b>fold-4</b>\n",
    "    ├── <b>models</b>\n",
    "    │      └── <b>best_balanced_accuracy</b>\n",
    "    │          └── model_best.pth.tar\n",
    "    └── <b>cnn_classification</b>\n",
    "           └── <b>best_balanced_accuracy</b>\n",
    "               └── validation_{patch|roi|slice}_level_prediction.tsv\n",
    "</pre>\n",
    "This file system is a part of the output of `clinicadl train` and `clinicadl classify` relies on three files:\n",
    "<ul>\n",
    "    <li> <code>commandline.json</code> contains all the options that were entered for training (type of input, architecture, preprocessing...).</li>\n",
    "    <li> <code>model_best.pth.tar</code> corresponds to the model selected when the best validation balanced accuracy was obtained.</li>\n",
    "    <li> <code>validation_{patch|roi|slice}_level_prediction.tsv</code> is specific to patch, roi and slice frameworks and is necessary to perform <b>soft-voting</b>  and find the label at the image level in unbiased way. Weighting the patches based on their performance of input data would bias the result as the classification framework would exploit knowledge of the test data.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: lightgreen; padding: 10px\">\n",
    "<p class=\"title\">Tip</p>\n",
    "    You can use your own previuolsy trained model (if you have used PyTorch\n",
    "    for that). Indeed, PyTorch stores model weights in a file with extension\n",
    "    <i>pth.tar</i>. You can place this file into the <i>models</i> folder and\n",
    "    try to follow the same structure that is described above. You also need to\n",
    "    fill a <i>commandline.json</i> file with all the parameters used during\n",
    "    the training (see <a\n",
    "    href=\"https://clinicadl.readthedocs.io/en/latest/Train/Introduction/#outputs\">ClinicaDL\n",
    "    documentation</a>) for further info.</p>\n",
    "</div>\n",
    "\n",
    ":::{tip}\n",
    "You can use your own previuolsy trained model (if you have used PyTorch for\n",
    "that). Indeed, PyTorch stores model weights in a file with extension\n",
    "`pth.tar`. You can place this file into the `models` folder and try to follow\n",
    "the same structure that is described above. You also need to fill a\n",
    "`commandline.json` file with all the parameters used during the training (see\n",
    "[ClinicaDL\n",
    "documentation](https://clinicadl.readthedocs.io/en/latest/Train/Introduction/#outputs)\n",
    "for further info.\n",
    ":::\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soft voting:</b><p>\n",
    "For classification tasks that take as input a part of the MRI volume\n",
    "(<i>patch, roi or slice</i>), an ensemble operation is needed to obtain the\n",
    "label at the image level.</p>\n",
    "<p>For example, size and stride of 50 voxels on linear preprocessing leads to\n",
    "the classification of 36 patches, but they are not all equally meaningful.\n",
    "Patches that are in the corners of the image are mainly composed of background\n",
    "and skull and may be misleading, whereas patches within the brain may be more\n",
    "useful.</p>\n",
    "<img src=\"./images/patches.png\">\n",
    "<p>Then the image-level probability of AD <i>p<sup>AD</sup></i> will be:</p>\n",
    "\n",
    "$$ p^{AD} = {\\sum_{i=0}^{35} bacc_i * p_i^{AD}}.$$\n",
    "\n",
    "where:<ul>\n",
    "<li> <i>p<sub>i</sub><sup>AD</sup></i> is the probability of AD for patch <i>i</i></li>\n",
    "<li> <i>bacc<sub>i</sub></i> is the validation balanced accuracy for patch <i>i</i></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8c793",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Download the pretrained models\n",
    "\n",
    ":::{warning} \n",
    "For the sake of the demonstration, this tutorial uses truncated versions of\n",
    "the models, containing only the first fold.\n",
    ":::\n",
    "\n",
    "In this notebook, we propose to use 4 specific models , all of them where trained to predict the classification task AD vs CN. (The experiment corresponding to the pretrained model in eTable 4 of the paper mentioned above is shown below):\n",
    "\n",
    "1. **3D image-level model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 3).\n",
    "2. **3D ROI-based model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 8).\n",
    "3. **3D patch-level model**, multi-cnn, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 14).\n",
    "4. **2D slice-level model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 18).\n",
    "\n",
    "Commands in the next code cell will automatically download and uncompress these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab85f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download here the pretrained models stored online\n",
    "# Model 1\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp3_splits_1.tar.gz  -o model_exp3_splits_1.tar.gz\n",
    "!tar xf model_exp3_splits_1.tar.gz\n",
    "\n",
    "# Model 2\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp8_splits_1.tar.gz  -o model_exp8_splits_1.tar.gz\n",
    "!tar xf model_exp8_splits_1.tar.gz\n",
    "\n",
    "# Model 3\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp14_splits_1.tar.gz  -o model_exp14_splits_1.tar.gz\n",
    "!tar xf model_exp14_splits_1.tar.gz\n",
    "\n",
    "# Model 4\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/models/v0.2.0/model_exp18_splits_1.tar.gz  -o model_exp18_splits_1.tar.gz"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
