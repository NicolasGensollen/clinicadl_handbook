
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Organization of neuroimaging data: the Brain Imaging Data Structure (BIDS) &#8212; Deep Learning Classification from brain MRI</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://aramislab.paris.inria.fr/clinicadl/tuto/notebooks/preprocessing.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to use clinicadl predict" href="inference.html" />
    <link rel="prev" title="Deep learning classification: application to neuroimaging" href="../deep_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-173464732-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logoAramis.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning Classification from brain MRI</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../clinical.html">
   Clinical context
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep_learning.html">
   Deep learning classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The basics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Prepare your neuroimaging data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="inference.html">
   Perform classification using pretrained models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Going further
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="tsvtools.html">
   Define your population
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="training.html">
   Train your own models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpretability.html">
   Interpret trained models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Architecture search
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="generate.html">
   Debug non-automated search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="random_search.html">
   Random search
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/preprocessing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/aramis-lab/tuto_clinicadl"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/aramis-lab/tuto_clinicadl/issues/new?title=Issue%20on%20page%20%2Fnotebooks/preprocessing.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/aramis-lab/tuto_clinicadl/edit/main/jupyter-book/notebooks/preprocessing.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/aramis-lab/tuto_clinicadl/main?urlpath=tree/jupyter-book/notebooks/preprocessing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/aramis-lab/tuto_clinicadl/blob/main/jupyter-book/notebooks/preprocessing.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-preprocessing-with-the-t1-linear-pipeline">
   Image preprocessing with the
   <code class="docutils literal notranslate">
    <span class="pre">
     t1-linear
    </span>
   </code>
   pipeline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quality-check-of-your-preprocessed-data">
   Quality check of your preprocessed data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensor-extraction-with-the-deeplearning-prepare-data-pipeline">
   Tensor extraction with the
   <code class="docutils literal notranslate">
    <span class="pre">
     deeplearning-prepare-data
    </span>
   </code>
   pipeline
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Organization of neuroimaging data: the Brain Imaging Data Structure (BIDS)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-preprocessing-with-the-t1-linear-pipeline">
   Image preprocessing with the
   <code class="docutils literal notranslate">
    <span class="pre">
     t1-linear
    </span>
   </code>
   pipeline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quality-check-of-your-preprocessed-data">
   Quality check of your preprocessed data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensor-extraction-with-the-deeplearning-prepare-data-pipeline">
   Tensor extraction with the
   <code class="docutils literal notranslate">
    <span class="pre">
     deeplearning-prepare-data
    </span>
   </code>
   pipeline
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Uncomment the next lines if running in Google Colab
!pip install clinicadl==0.2.1
!/bin/bash -c &quot;$(curl -k https://aramislab.paris.inria.fr/files/software/scripts/install_conda_ants.sh)&quot;
# from os import environ
# environ[&#39;ANTSPATH&#39;]=&quot;/usr/local/bin&quot;
</pre></div>
</div>
</div>
</div>
<p>Prepare your neuroimaging data</p>
<p>Different steps to perform before training your model or performing classification. In this notebook, we will see how to:</p>
<ol class="simple">
<li><p><strong>Organize</strong> your neuroimaging data.</p></li>
<li><p><strong>Preprocess</strong> of your neuroimaging data.</p></li>
<li><p>Check the preprocessing <strong>quality check</strong>.</p></li>
<li><p><strong>Extract tensors</strong> from your preprocessed data.</p></li>
</ol>
<div class="tex2jax_ignore mathjax_ignore section" id="organization-of-neuroimaging-data-the-brain-imaging-data-structure-bids">
<h1>Organization of neuroimaging data: the Brain Imaging Data Structure (BIDS)<a class="headerlink" href="#organization-of-neuroimaging-data-the-brain-imaging-data-structure-bids" title="Permalink to this headline">¶</a></h1>
<p>Before processing your neuroimaging data, several steps may be needed. These steps can include converting the images to a format readable by neuroimaging software tools (e.g. converting to NIfTI) and organizing your files in a specific way. Several tools will require that your clinical and imaging data follow the <strong>Brain Imaging Data Structure (BIDS)</strong> <a class="reference external" href="https://doi.org/10.1038/sdata.2016.44">(Gorgolewski et al., 2016)</a>. The BIDS standard is based on a file hierarchy rather than on a database management system, thus facilitating its deployment. Thanks to its clear and simple way to describe neuroimaging and behavioral data, it has been easily adopted by the neuroimaging community. Organizing a dataset following the BIDS hierarchy simplifies the execution of neuroimaging software tools.</p>
<p>Here is a general overview of the BIDS structure. If you need more details, please check the <a class="reference external" href="https://bids-specification.readthedocs.io/en/latest/">documentation</a> on the <a class="reference external" href="http://bids.neuroimaging.io/">website</a>.</p>
<pre>
BIDS_Dataset/
├── participants.tsv
├── sub-CLNC01/
│   │   ├── ses-M00/
│   │   │   └── anat/
│   │   │       └── <b>sub-CLNC01_ses-M00_T1w.nii.gz</b>
│   │   └── sub-CLNC01_sessions.tsv
├── sub-CLNC02/
│   │   ├── ses-M00/
│   │   │   └── anat/
│   │   │       └── <b>sub-CLNC02_ses-M00_T1w.nii.gz</b>
│   │   └── sub-CLNC02_sessions.tsv
└──  ...
</pre>
<p>The OASIS dataset contains imaging data in ANALYZE format and does not provide a BIDS version of the data. To solve this issue, <a class="reference external" href="http://www.clinica.run/doc/Converters/OASIS2BIDS/">Clinica provides a converter</a> to automatically convert ANALYZE files into NIfTI following the BIDS standard.</p>
<p>A command line instruction is enough to get the data in BIDS format:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>clinica convert oasis-to-bids &lt;dataset_directory&gt; &lt;clinical_data_directory&gt; &lt;bids_directory&gt;
</pre></div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dataset_directory</span></code> is the path to the original OASIS images’ directory;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">clinical_data_directory</span></code> is the path to the directory containing the <code class="docutils literal notranslate"><span class="pre">oasis_cross-sectional.csv</span></code> file;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bids_directory</span></code> is the path to the output directory, where the BIDS-converted version of OASIS will be stored.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Download the example dataset of 4 images
!curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisDatabase.tar.gz -o OasisDatabase.tar.gz
!tar xf OasisDatabase.tar.gz
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Convert the example dataset to BIDS
!clinica convert oasis-to-bids OasisDatabase/RawData OasisDatabase/ClinicalData OasisBids_example
</pre></div>
</div>
</div>
</div>
<p><a id='preprocessing:t1-linear'></a></p>
<div class="section" id="image-preprocessing-with-the-t1-linear-pipeline">
<h2>Image preprocessing with the <code class="docutils literal notranslate"><span class="pre">t1-linear</span></code> pipeline<a class="headerlink" href="#image-preprocessing-with-the-t1-linear-pipeline" title="Permalink to this headline">¶</a></h2>
<p>Although convolutional neural networks (CNNs) have the potential to extract low-to-high level features from raw images, a proper image preprocessing procedure is fundamental to ensure a good classification performance (in particular for Alzheimer’s disease (AD) classification where datasets are relatively small).</p>
<p>In the context of deep learning-based classification, image preprocessing procedures often include:</p>
<ul class="simple">
<li><p><strong>Bias field correction:</strong> MR images can be corrupted by a low frequency and smooth signal caused by magnetic field inhomogeneities. This bias field induces variations in the intensity of the same tissue in different locations of the image, which deteriorates the performance of image analysis algorithms such as registration.</p></li>
<li><p><strong>Image registration:</strong> Medical image registration consists of spatially aligning two or more images, either globally (rigid and affine registration) or locally (non-rigid registration), so that voxels in corresponding positions contain comparable information.</p></li>
</ul>
<p>Finally, a <strong>Cropping</strong> of the registered images can be performed to remove the background and to reduce the computing power required when training deep learning models.</p>
<p>For this tutorial, we propose a “minimal preprocessing” (as described in <a class="reference external" href="https://doi.org/10.1016/j.media.2020.101694">(Wen et al., 2020)</a>) implemented in the <a class="reference external" href="http://www.clinica.run/doc/Pipelines/T1_Linear/"><code class="docutils literal notranslate"><span class="pre">t1-linear</span></code> pipeline</a> using the <a class="reference external" href="http://stnava.github.io/ANTs/">ANTs</a> software package <a class="reference external" href="https://doi.org/10.3389/fninf.2014.00044">(Avants et al., 2014)</a>. This preprocessing includes:</p>
<ul class="simple">
<li><p><strong>Bias field correction</strong> using the N4ITK method <a class="reference external" href="https://doi.org/10.1109/TMI.2010.2046908">(Tustison et al., 2010)</a>.</p></li>
<li><p><strong>Affine registration</strong> to the MNI152NLin2009cSym template (Fonov et al., <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2010.07.033">2011</a>, <a class="reference external" href="https://doi.org/10.1016/S1053-8119(09)70884-5">2009</a> ) in MNI space with the SyN algorithm <a class="reference external" href="https://doi.org/10.1016/j.media.2007.06.004">(Avants et al., 2008)</a>.</p></li>
<li><p><strong>Cropping</strong> resulting in final images of size 169×208×179 with 1 mm3 isotropic voxels.</p></li>
</ul>
<p>If you run this notebook locally, please check that ANTs is correctly installed. If it is not the case, uncomment the three last lines of the first cell and run it.</p>
<p>These steps can be run with this simple command line:</p>
<div class="highlight-Text notranslate"><div class="highlight"><pre><span></span>clinica run t1-linear &lt;bids_directory&gt; &lt;caps_directory&gt;
</pre></div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">bids_directory</span></code> is the input folder containing the dataset in a <a class="reference external" href="http://www.clinica.run/doc/BIDS/">BIDS</a> hierarchy,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">caps_directory</span></code> is the output folder containing the results in a <a class="reference external" href="http://www.clinica.run/doc/CAPS/">CAPS</a> hierarchy.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The following command can take some time to execute, depending on the
configuration of your host machine. Running in a classical <strong>Colab</strong> instance
can take up to 30 min.</p>
<p>We will increase a little bit the computation capacity using 2 cores with the
<code class="docutils literal notranslate"><span class="pre">--n_procs</span> <span class="pre">2</span></code> flag. Since there are 4 images, you can set <code class="docutils literal notranslate"><span class="pre">--n_procs</span> <span class="pre">4</span></code> if
your computer can handle this.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!clinica run t1-linear ./OasisBids_example ./OasisCaps_example --n_procs 2
</pre></div>
</div>
</div>
</div>
<p>Once the pipeline has been run, the necessary outputs for the next steps are saved using a specific suffix:
<code class="docutils literal notranslate"><span class="pre">_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The registration algorithm provided by ANTs exposes some reproducibility issues
when running in different environments. The outputs are “visually” very close
but not exactly the same. For further information and some clues on how to
reduce the variability please read this
<a class="reference external" href="https://github.com/ANTsX/ANTs/wiki/antsRegistration-reproducibility-issues">page</a>.</p>
</div>
<p>For example, processed images from our dataset are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn</span> <span class="kn">import</span> <span class="n">plotting</span>

<span class="n">suffix</span> <span class="o">=</span> <span class="s1">&#39;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz&#39;</span>

<span class="n">sub1</span> <span class="o">=</span> <span class="s1">&#39;OasisCaps_example/subjects/sub-OASIS10016/ses-M00/t1_linear/sub-OASIS10016_ses-M00&#39;</span> <span class="o">+</span> <span class="n">suffix</span> 
<span class="n">sub2</span> <span class="o">=</span> <span class="s1">&#39;OasisCaps_example/subjects/sub-OASIS10109/ses-M00/t1_linear/sub-OASIS10109_ses-M00&#39;</span> <span class="o">+</span> <span class="n">suffix</span>
<span class="n">sub3</span> <span class="o">=</span> <span class="s1">&#39;OasisCaps_example/subjects/sub-OASIS10304/ses-M00/t1_linear/sub-OASIS10304_ses-M00&#39;</span> <span class="o">+</span> <span class="n">suffix</span>
<span class="n">sub4</span> <span class="o">=</span> <span class="s1">&#39;OasisCaps_example/subjects/sub-OASIS10363/ses-M00/t1_linear/sub-OASIS10363_ses-M00&#39;</span> <span class="o">+</span> <span class="n">suffix</span>

<span class="n">plotting</span><span class="o">.</span><span class="n">plot_anat</span><span class="p">(</span><span class="n">sub1</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;sub-OASIS10016&quot;</span><span class="p">)</span>
<span class="n">plotting</span><span class="o">.</span><span class="n">plot_anat</span><span class="p">(</span><span class="n">sub2</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;sub-OASIS10109&quot;</span><span class="p">)</span>
<span class="n">plotting</span><span class="o">.</span><span class="n">plot_anat</span><span class="p">(</span><span class="n">sub3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;sub-OASIS10304&quot;</span><span class="p">)</span>
<span class="n">plotting</span><span class="o">.</span><span class="n">plot_anat</span><span class="p">(</span><span class="n">sub4</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;sub-OASIS10363&quot;</span><span class="p">)</span>
<span class="n">plotting</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>From the visualization above, we can see that the last two images have some
missing skin voxels on top of the brain i.e. these images are slightly cropped.
Besides, we did not compare them to the <a class="reference external" href="https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html">MNI152NLin2009cSym
template</a>
to evaluate the quality of the registration.</p>
<p>OASIS-1 dataset contains 416 images so quality check of the whole dataset can be
very time consuming. The next section gives you some ideas on how to keep only
images correctly preprocessed, when running in a large dataset.</p>
</div>
<div class="section" id="quality-check-of-your-preprocessed-data">
<h2>Quality check of your preprocessed data<a class="headerlink" href="#quality-check-of-your-preprocessed-data" title="Permalink to this headline">¶</a></h2>
<p>To automatically assess the quality of the preprocessing, we propose to use a
pretrained network which learnt to classify images that are adequately
registered to a template from others for which the registration failed. This
procedure is adaptated from <a class="reference external" href="https://www.biorxiv.org/content/10.1101/303487v1">(Fonov et al,
2018)</a>, using their
pretrained models. The original code of <a class="reference external" href="https://www.biorxiv.org/content/10.1101/303487v1">(Fonov et al,
2018)</a> can be found on
<a class="reference external" href="https://github.com/vfonov/deep-qc">GitHub</a>.</p>
<p>The quality check can be run with the following command line:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clinicadl</span> <span class="n">quality</span><span class="o">-</span><span class="n">check</span> <span class="o">&lt;</span><span class="n">preprocessing</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">caps_directory</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">output_path</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">preprocessing</span></code> corresponds to the preprocessing pipeline whose outputs will be checked (<code class="docutils literal notranslate"><span class="pre">t1-linear</span></code>),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">caps_directory</span></code> is the folder containing the results of the <code class="docutils literal notranslate"><span class="pre">t1-linear</span></code> pipeline in a <a class="reference external" href="http://www.clinica.run/doc/CAPS/Introduction/">CAPS</a> hierarchy,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_path</span></code> is the path to the output TSV file containing QC results.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!clinicadl preprocessing quality-check t1-linear OasisCaps_example QC_result.tsv
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># After execution of the quality check procedure, the `QC_result.tsv` file will</span>
<span class="c1"># look like this:</span>

<span class="c1"># | participant_id | session_id | pass_probability   | pass  |</span>
<span class="c1"># |----------------| -----------|--------------------|-------|</span>
<span class="c1"># | sub-OASIS10016 | ses-M00    | 0.9936990737915039 | True  |</span>
<span class="c1"># | sub-OASIS10109 | ses-M00    | 0.9772214889526367 | True  |</span>
<span class="c1"># | sub-OASIS10363 | ses-M00    | 0.7292165160179138 | True  |</span>
<span class="c1"># | sub-OASIS10304 | ses-M00    | 0.1549495905637741 | &lt;font color=&quot;red&quot;&gt;False&lt;/font&gt; |</span>

<span class="c1"># Based on this TSV file, participant `OASIS10304` should be discarded for the</span>
<span class="c1"># rest of your anlysis. If you compare its registration with [MNI152NLin2009cSym</span>
<span class="c1"># template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html),</span>
<span class="c1"># you will see that temporal regions are misaligned as well as occipital regions</span>
<span class="c1"># and cerebellum leading to this low probabilty value. ;-)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tensor-extraction-with-the-deeplearning-prepare-data-pipeline">
<h2>Tensor extraction with the <code class="docutils literal notranslate"><span class="pre">deeplearning-prepare-data</span></code> pipeline<a class="headerlink" href="#tensor-extraction-with-the-deeplearning-prepare-data-pipeline" title="Permalink to this headline">¶</a></h2>
<p>Once the dataset has been preprocessed, we need to obtain files suited for the
training phase.  This task can be performed using the <a class="reference external" href="http://www.clinica.run/doc/Pipelines/DeepLearning_PrepareData/">Clinica
<code class="docutils literal notranslate"><span class="pre">deeplearning-prepare-data</span></code>
pipeline</a>.</p>
<p>This pipeline prepares images generated by Clinica to be used with the PyTorch
deep learning library <a class="reference external" href="https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library">(Paszke et al.,
2019)</a>.
Three types of tensors are proposed: 3D images, 3D patches or 2D slices.</p>
<p>This pipeline selects the preprocessed images, extract the “tensors”, and write
them as output files for the entire images, for each slice or for each patch.</p>
<p>You simply need to type the following command line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>clinica run deeplearning-prepare-data &lt;caps_directory&gt; &lt;tensor_format&gt;
</pre></div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">caps_directory</span></code> is the folder containing the results of the <a class="reference external" href="#Preprocess-raw-images-with-t1-linear-pipeline"><code class="docutils literal notranslate"><span class="pre">t1-linear</span></code> pipeline</a> and the output of the present command, both in a CAPS hierarchy.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor_format</span></code> is the format of the extracted tensors. You can choose between <code class="docutils literal notranslate"><span class="pre">image</span></code> to convert to PyTorch tensor the whole 3D image, <code class="docutils literal notranslate"><span class="pre">patch</span></code> to extract 3D patches and <code class="docutils literal notranslate"><span class="pre">slice</span></code> to extract 2D slices from the image.</p></li>
</ul>
<p>Output files are stored into a new folder (inside the CAPS) and follows a struture like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>deeplearning_prepare_data
├── image_based
│   └── t1_linear
│       └── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt
├── patch_based
│   └── t1_linear
│       ├── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-0_T1w.pt
│       ├── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-1_T1w.pt
│       ├── ...
│       └── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-N_T1w.pt
└── slice_based
    └── t1_linear
        ├── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-0_T1w.pt
        ├── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-1_T1w.pt
        ├── ...
        ├── sub-&lt;participant_label&gt;_ses-&lt;session_label&gt;_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-N_T1w.pt
</pre></div>
</div>
<p>In short, there is a folder for each feature (<strong>image, slice or patch</strong>) and
inside the numbered tensor files with the corresponding feature.</p>
<div class="alert alert-info">
<p><strong>Note:</strong> You can choose to only extract the tensors for the whole images
*(<code class="docutils literal notranslate"><span class="pre">clinica</span> <span class="pre">run</span> <span class="pre">deeplearning-prepare-data</span> <span class="pre">&lt;caps_directory&gt;</span> <span class="pre">image</span></code> ) and continue
*working with one single file per subject/session.</p>
<p>The package <code class="docutils literal notranslate"><span class="pre">clinicadl</span></code> is able to extract patches or slices <em>on-the-fly</em> (from
one single file) when running training or inference tasks. The downside of this
approach is that, depending on the size of your dataset, you have to make sure
that you have enough memory ressources in your GPU card to host the full
images/tensors for all your data.</p>
<p>If the memory size of the GPU card you use is too small, we suggest you to
extract the patches and/or the slices using the proper <code class="docutils literal notranslate"><span class="pre">tensor_format</span></code> option of
the command described above.</p>
</div><p>(If you failed to obtain the preprocessing using the <code class="docutils literal notranslate"><span class="pre">t1-linear</span></code> pipeline,
please uncomment the next cell)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisCaps1.tar.gz -o OasisCaps1.tar.gz
!tar xf OasisCaps1.tar.gz
</pre></div>
</div>
</div>
</div>
<p>To perform the feature extraction for our dataset, run the following cell:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!clinica run deeplearning-prepare-data ./OasisCaps_example t1-linear image
</pre></div>
</div>
</div>
</div>
<p>At the end of this command, a new directory named <code class="docutils literal notranslate"><span class="pre">deeplearning_prepare_data</span></code> is
created inside each subject/session of the CAPS structure. We can easily verify:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!tree -L 3 ./OasisCaps_example/subjects/sub-OASIS10*/ses-M00/deeplearning_prepare_data/
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../deep_learning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Deep learning classification: application to neuroimaging</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="inference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to use <code class="docutils literal notranslate"><span class="pre">clinicadl</span> <span class="pre">predict</span></code></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The Aramis Team<br/>
    
        &copy; Copyright 2020-2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>